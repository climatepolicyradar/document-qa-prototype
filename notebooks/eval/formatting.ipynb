{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting improvements\n",
    "\n",
    "The goal of this notebook is to improve on the formatting evaluator, and identify the reason for the high number of rules broken that are flagged by it.<br>\n",
    "This was an iterative process, looking at the results, and fixing any potential issues with the evaluator.<br>\n",
    "The result files are under `s3://project-rag/data/eval/formatting/`. They were obtained by running different versions of the formatting evaluator ont the `final_2_all.jsonl` dataset (except for `system-responds-results` which is the SystemResponse evaluator.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from textwrap import wrap\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "sys.path.append(Path.cwd().parent.parent.as_posix())\n",
    "\n",
    "from src.online.data_models import EndToEndGeneration\n",
    "from src.evaluation.system_response import SystemResponse\n",
    "from src.evaluation.formatting import Formatting\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_response_evaluator = SystemResponse()\n",
    "formatting_evaluator = Formatting()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gens = pd.read_json(\"s3://project-rag/data/dataset_generation/unece_sprint/final_2_all.jsonl\", lines=True)\n",
    "\n",
    "all_gens[\"gen_uuid\"] = all_gens[\"generation\"].apply(lambda x: EndToEndGeneration.model_validate(x).uuid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"s3://project-rag/data/eval/formatting/formatting-results-with-comments-multiple-latest.jsonl\", lines=True)\n",
    "\n",
    "system_responds_df = pd.read_json(\"s3://project-rag/data/eval/formatting/system-responds-results.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapped(text: str) -> str:\n",
    "    return \"\\n\".join(wrap(text.replace(\"\\n\", \"NNNNN\"), width=100)).replace(\"NNNNN\", \"\\n\")\n",
    "\n",
    "\n",
    "def print_example(example: EndToEndGeneration):\n",
    "    print(\"=\"*100)\n",
    "    print(wrapped(example.rag_request.query))\n",
    "    print(\"-\"*100)\n",
    "    print(wrapped(example.rag_response.text))\n",
    "    print(\"-\"*100)\n",
    "    print(wrapped(example.rag_response.retrieved_passages_as_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System did not respond: 2747\n",
      "System said it wouldn't respond, but later it did: 735\n"
     ]
    }
   ],
   "source": [
    "system_does_not_respond = system_responds_df[(system_responds_df[\"score\"] == 0.0) & (system_responds_df[\"comment\"].isna())].gen_uuid.tolist()\n",
    "\n",
    "print(\"System did not respond:\", len(system_does_not_respond))\n",
    "\n",
    "system_half_responds = system_responds_df[(system_responds_df[\"score\"] == 0.5) & (system_responds_df[\"comment\"].isna())].gen_uuid.tolist()\n",
    "\n",
    "print(\"System said it wouldn't respond, but later it did:\", len(system_half_responds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rule break counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('no_citation', 3450),\n",
       " ('fictitious_citation', 1179),\n",
       " ('rag_response_is_none', 524),\n",
       " ('quotations_not_verbatim', 347),\n",
       " ('answer_not_english', 5)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total rule break counts:\")\n",
    "Counter(chain.from_iterable(df[~df.comments.isna()].comments.tolist())).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule break counts without system-does-not-respond:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('no_citation', 1280),\n",
       " ('fictitious_citation', 1145),\n",
       " ('quotations_not_verbatim', 314),\n",
       " ('answer_not_english', 5)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rule break counts without system-does-not-respond:\")\n",
    "Counter(chain.from_iterable(df[(~df.comments.isna()) & (~df.gen_uuid.isin(system_does_not_respond))].comments.tolist())).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule break counts where system responds:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('fictitious_citation', 1116),\n",
       " ('no_citation', 585),\n",
       " ('quotations_not_verbatim', 102),\n",
       " ('answer_not_english', 5)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Rule break counts where system responds:\")\n",
    "\n",
    "Counter(\n",
    "    chain.from_iterable(\n",
    "        df[\n",
    "            (~df.comments.isna()) & \n",
    "            (~df.gen_uuid.isin(system_does_not_respond + system_half_responds))\n",
    "        ].comments.tolist()\n",
    "    )\n",
    ").most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the `no_citation` cases, we see that:\n",
    "- most of them are legit missed citations  (e.g. `aa0596d9965f6f84973e40dcd40be45d`, `a84ef8ba73bc4332e71fe4927a68c416`, `7c170ed3940971fa0e6c33d3eec4d67c`)\n",
    "- some are citation formatting issues, e.g. `[Source 2]` which is not our required citation\n",
    "- some are no response cases differently worded (e.g. `d01fb208fe258d3483eef45eced6621f`, `28c2189d1140f196158ce45b0b586e3b`)\n",
    "- and there are some False Positives too where the first sentence is weirdly formatted, but the later bulletpoints are cited (e.g. `8cdb8b0ceba78275839377d2d15a16a1`, or `c94fb4e53a9305de91ccfbcc2c090299` due to sentence splitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fictitious_citation` cases show us the known issue of:\n",
    "- citations are shifted by one, and hence the system references `[0]` (which doesn't exist) instead of `[1]`. (e.g. `fd8edd6c2e34c3a1c5ea4884ecb4532b`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `quotations_not_verbatim` cases are:\n",
    "- catching hallucinations (e.g. `5a3099c0eec7822f4efdbb3ad687d4e3`, `cb14aa8c319acd54c03a4a81ff95195d`, `a8359b155f6333b23a609e9be21a23ee`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`answer_not_english` scenario:\n",
    "- 3/5 false positives\n",
    "- 2/5 with text that's mostly not in english (`caed0a8b3b23fc3d3d21ac2be35f4d8f` and `5cf2fb63ba78db4bb07d0f8d6c409078`) and are of low quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5cf2fb63ba78db4bb07d0f8d6c409078\n",
      "====================================================================================================\n",
      "Does the document mention any specific dates?\n",
      "----------------------------------------------------------------------------------------------------\n",
      "- Yes, the document mentions the following dates:\n",
      "    - 7 Jadishna Shakitau Ruretat [0]\n",
      "\n",
      "- 240 Naka [1]\n",
      "    - 2020/922 Yagyuna [1]\n",
      "    - 4498 Mashmam Karfuk [1]\n",
      "    - 2020 (222\n",
      "Kagyan [1]\n",
      "    - 5838 Jabitar Tashtashakti [1]\n",
      "    - 24, 2000 Trithvaja, Ekkranti [2]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "[1]: &quot;(S) Kamiyatanal Dhyaazbhajan Patra Jama Shat Nirsadaan Jana Thaaritan Nimnavid (Yagaaz 7\n",
      "Jadishna Shakitau Ruretat, Yasha :-\n",
      "[2]: Naka, 240] (Sakranti, 2020/922 Yagyuna, 4498\n",
      "[3]:\n",
      "Panibich marya kafreshak kk tighaniye\n",
      "====================================================================================================\n",
      "[['answer_not_english']]\n"
     ]
    }
   ],
   "source": [
    "no_response_formatting = df[(~df.comments.isna()) & (~df.gen_uuid.isin(system_does_not_respond + system_half_responds))]\n",
    "\n",
    "_ids = no_response_formatting[no_response_formatting.comments.apply(lambda x: \"answer_not_english\" in x)].gen_uuid.tolist()\n",
    "\n",
    "item = _ids[3]\n",
    "\n",
    "e2e = EndToEndGeneration.model_validate(all_gens[all_gens.gen_uuid == item][\"generation\"].tolist()[0])\n",
    "\n",
    "print(e2e.uuid)\n",
    "print_example(e2e)\n",
    "print(\"=\"*100)\n",
    "print(df[df.gen_uuid.isin({item})].comments.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
